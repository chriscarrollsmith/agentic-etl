---
description: Scraping Internet data with Python
globs: 
alwaysApply: false
---
## 1. Lazy Self-Installation of Dependencies

If you are working in a repository without a Python virtual environment or want a self-contained script, you can use the “uv” tool. The script’s header tells “uv” which Python version and dependencies are required:

```python
#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "playwright",
# ]
# ///
```

Run it with:

```bash
uv run main.py
```

## 2. Using Playwright with a Realistic User Agent

Many websites detect and block requests that have default or no user agents, which can happen with libraries like requests or Playwright without customization. A realistic user agent can help mitigate this:

```python
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    browser = p.chromium.launch(headless=True)
    context = browser.new_context(
        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                   "(KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36"
    )
    page = context.new_page()

    page.goto("https://example.com")
    print(page.title())

    browser.close()
```

## 3. Randomized Delays to Avoid Rate Limiting

When scraping many pages, randomizing the delay between requests helps mimic human browsing behavior. This makes it less likely that you’ll get flagged or blocked:

```python
import time
import random

def random_wait(min_sec=5, max_sec=10):
    time.sleep(random.uniform(min_sec, max_sec))

for i in range(5):
    random_wait()
    print(f"Scrape iteration {i+1}")
```

## 4. Scraping Dynamically Loaded Data with Browser Automation

Some sites load content via JavaScript after the initial HTML response. Tools like Playwright let you evaluate JavaScript directly in the browser context to scrape accurately:

```python
data = page.evaluate("""() => {
    const items = [];
    document.querySelectorAll('div.item').forEach(elem => {
        const titleElem = elem.querySelector('h2');
        const linkElem = elem.querySelector('a');
        if (titleElem && linkElem) {
            items.push({
                title: titleElem.innerText,
                url: linkElem.href
            });
        }
    });
    return items;
}""")
print(data)
```

## 5. Detecting Actual File Types

Sometimes, the HTTP headers can give misleading MIME types (e.g., claiming the document is HTML when it is actually PDF). You can detect the real file type by examining the file signature.

Example using the python-magic library:

```python
import requests
import magic

url = "https://example.com/document"
  
with requests.get(url, stream=True) as response:
    content_type = response.headers.get('Content-Type', 'unknown')
    # Peek at some of the incoming bytes
    first_bytes = response.raw.read(2048)  
    true_mime = magic.from_buffer(first_bytes, mime=True)
      
    print("Header MIME Type:", content_type)
    print("Detected MIME Type:", true_mime)
```
  
This helps you decide how to store or further process a downloaded file (e.g., what file extension to save it with).

## 6. Paginating and Retrying Upon Errors

Many sites spread articles across multiple pages, and network issues or timeouts can occur. Looping through pages and implementing retries on failures helps you gather a complete dataset:

```python
MAX_RETRIES = 3

for page_num in range(1, 6):  # 5 pages
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            url = f"https://example.com/page={page_num}"
            page.goto(url, timeout=10000)  # 10s timeout

            # Extract data here ...
            print(f"Page {page_num} data extracted successfully!")
            break  # Exit retry loop if successful
        except Exception as err:
            print(f"Error on page {page_num}, attempt {attempt}: {err}")
            if attempt == MAX_RETRIES:
                print("Skipping this page after reaching max retries.")
```

## 7. Handling Errors and Validating and Organizing the Results

- Fail gracefully by catching exceptions, logging errors, and continuing the workflow.
- Inspect each field (e.g., URLs for download links) when done to catch any issues.
- Store any relevant metadata (like MIME type).

## 8. Skipping the Workflow if the Data Is Already Available

Before running your scraping workflow, check if the relevant JSON objects already exist and have non-null values. If it is, you can skip the workflow to save time and resources:

```python
if os.path.exists("output.json"):
    with open("output.json", "r") as f:
        data = json.load(f)
    for item in data:
        if item["content"] is not None:
            print("Data already exists, skipping item")
            continue
```